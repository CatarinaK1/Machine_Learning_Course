{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import nltk\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Topic detection\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RegExps "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RegExp: Stands for Regular Expression, it is way to analyze a sequence of characters and find patterns. \n",
    "RegEx can be used to check if a string contains the specified search pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "YES! We have a match!\n",
      "['ai', 'ai', 'ai', 'ai']\n"
     ]
    }
   ],
   "source": [
    "# import the necessary library\n",
    "import re\n",
    "\n",
    "#create a text where we will analyse and find patterns\n",
    "text = \"Its raining here in Spain. The air feels cold. hope we can sail today\"\n",
    "\n",
    "#Search in the text if \"ai\" can be found\n",
    "x = re.findall(\"ai\", text)\n",
    "\n",
    "\n",
    "#Search the string to see if it starts with \"Its\" and ends with \"today\":\n",
    "z = re.search(\"^Its.*today$\", text) \n",
    "\n",
    "# In case we have a match print\n",
    "if z:\n",
    "  print(\"YES! We have a match!\")\n",
    "else:\n",
    "  print(\"No match\")\n",
    "\n",
    "#print the list where the information was stored\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bow  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bag of Words is a method to extract features from text documents.This model is used to preprocess the text by converting it into a bag of words, which keeps a count of the total occurrences of most frequently used words.\n",
    "\n",
    "These features can be used for training machine learning algorithms. It creates a vocabulary of all the unique words occurring in all the documents in the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bag-of-Words Matrix:\n",
      "[[0 0 0 0 1 1 1 0 0 1 1 0 1 1 2]\n",
      " [0 0 1 1 0 1 0 0 0 0 0 1 0 0 2]\n",
      " [1 1 0 0 0 1 1 1 1 0 0 0 0 0 2]]\n",
      "\n",
      "Feature Names:\n",
      "['and' 'are' 'at' 'barked' 'brown' 'dog' 'fox' 'friends' 'good' 'jumped'\n",
      " 'lazy' 'moon' 'over' 'quick' 'the']\n"
     ]
    }
   ],
   "source": [
    "# Import the necessary library\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import nltk \n",
    "import numpy as np \n",
    "\n",
    "documents = [\n",
    "    \"The quick brown fox jumped over the lazy dog.\",\n",
    "    \"The dog barked at the moon.\",\n",
    "    \"The fox and the dog are good friends.\"\n",
    "]\n",
    "\n",
    "# Create a CountVectorizer instance\n",
    "#The CountVectorizer from scikit-learn is used to convert a collection of text documents into a matrix of token counts\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# Fit the vectorizer to the documents and transform them into a bag-of-words representation\n",
    "#The fit_transform method fits the model to the documents and transforms the documents into a bag-of-words matrix.\n",
    "bow_matrix = vectorizer.fit_transform(documents)\n",
    "\n",
    "\n",
    "# Get the feature names (words) in the bag-of-words representation\n",
    "#The get_feature_names_out method returns the feature names (words) in the bag-of-words representation.\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "\n",
    "# Print the bag-of-words matrix and feature names\n",
    "print(\"Bag-of-Words Matrix:\")\n",
    "print(bow_matrix.toarray())\n",
    "print(\"\\nFeature Names:\")\n",
    "print(feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenization: in Natural Language Processing (NLP) and machine learning, refers to the process of converting a sequence of text into smaller parts, known as tokens. \n",
    "\n",
    "These tokens can be as small as characters or as long as words. The primary reason this process matters is that it helps machines understand human language by breaking it down into bite-sized pieces, which are easier to analyze."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download NLTK resources (you only need to do this once)\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Tokenization', 'is', 'important', 'for', 'NLP', '.', 'It', 'helps', 'machines', 'understand', 'documents', 'and', 'text', 'by', 'breaking', 'it', 'down']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Input text\n",
    "text = \"Tokenization is important for NLP. It helps machines understand documents and text by breaking it down\"\n",
    "\n",
    "# Tokenize the text into words\n",
    "tokens = word_tokenize(text)\n",
    "\n",
    "# Print the tokens\n",
    "print(tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatization   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lemmatization: It is the process of grouping together the different inflected forms of a word so they can be analyzed as a single item.\n",
    "It works by linking words with similar meanings to one word.  reducing words to their base or root form, known as the lemma. \n",
    "\n",
    "For example, the lemma of the words \"running,\" \"ran,\" and \"runs\" is \"run.\" Lemmatization is often used in natural language processing (NLP) to standardize words and reduce them to their base form for better analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download NLTK resources (you only need to do this once)\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rocks : rock\n",
      "corpora : corpus\n",
      "better : good\n",
      "running : run\n",
      "largest : large\n",
      "trying : try\n"
     ]
    }
   ],
   "source": [
    "# Import necessary library\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "#Here, an instance of the WordNetLemmatizer is created. This object (lemmatizer) will be used to perform lemmatization on words.\n",
    "lemmatizer = WordNetLemmatizer()\n",
    " \n",
    "#This line prints the result of lemmatizing the word \"rocks\" using the lemmatizer. \n",
    "# The output will be \"rock\" because \"rocks\" is lemmatized to its base form.\n",
    "\n",
    "print(\"rocks :\", lemmatizer.lemmatize(\"rocks\"))\n",
    "print(\"corpora :\", lemmatizer.lemmatize(\"corpora\"))\n",
    " \n",
    "#This line prints the result of lemmatizing the word \"better\" with the additional information \n",
    "# that it is an adjective (denoted by the pos=\"a\" argument). \n",
    "# The output will be \"good\" because the lemma of the comparative adjective \"better\" is \"good.\"\n",
    "# a denotes adjective in \"pos\"\n",
    "print(\"better :\", lemmatizer.lemmatize(\"better\", pos =\"a\"))\n",
    "\n",
    "\n",
    "#The output will be \"run\" because \"running\" is the present participle form of the verb, and lemmatization reduces it to the base form\n",
    "print(\"running :\", lemmatizer.lemmatize(\"running\", pos=\"v\"))\n",
    "\n",
    "\n",
    "print(\"largest :\", lemmatizer.lemmatize(\"largest\", pos=\"a\"))\n",
    "\n",
    "print(\"trying :\", lemmatizer.lemmatize(\"trying\", pos=\"v\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tfidf   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "words please"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Named entity recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "feed words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Sentiment analysis\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
