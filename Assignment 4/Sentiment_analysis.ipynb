{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import nltk\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Topic detection\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RegExps "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RegExp: Stands for Regular Expression, it is way to analyze a sequence of characters and find patterns. \n",
    "RegEx can be used to check if a string contains the specified search pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "YES! We have a match!\n",
      "['ai', 'ai', 'ai', 'ai']\n"
     ]
    }
   ],
   "source": [
    "# import the necessary library\n",
    "import re\n",
    "\n",
    "#create a text where we will analyse and find patterns\n",
    "text = \"Its raining here in Spain. The air feels cold. hope we can sail today\"\n",
    "\n",
    "#Search in the text if \"ai\" can be found\n",
    "x = re.findall(\"ai\", text)\n",
    "\n",
    "\n",
    "#Search the string to see if it starts with \"Its\" and ends with \"today\":\n",
    "z = re.search(\"^Its.*today$\", text) \n",
    "\n",
    "# In case we have a match print\n",
    "if z:\n",
    "  print(\"YES! We have a match!\")\n",
    "else:\n",
    "  print(\"No match\")\n",
    "\n",
    "#print the list where the information was stored\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bow  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bag of Words is a method to extract features from text documents.This model is used to preprocess the text by converting it into a bag of words, which keeps a count of the total occurrences of most frequently used words.\n",
    "\n",
    "These features can be used for training machine learning algorithms. It creates a vocabulary of all the unique words occurring in all the documents in the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bag-of-Words Matrix:\n",
      "[[0 0 0 0 1 1 1 0 0 1 1 0 1 1 2]\n",
      " [0 0 1 1 0 1 0 0 0 0 0 1 0 0 2]\n",
      " [1 1 0 0 0 1 1 1 1 0 0 0 0 0 2]]\n",
      "\n",
      "Feature Names:\n",
      "['and' 'are' 'at' 'barked' 'brown' 'dog' 'fox' 'friends' 'good' 'jumped'\n",
      " 'lazy' 'moon' 'over' 'quick' 'the']\n"
     ]
    }
   ],
   "source": [
    "# Import the necessary library\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import nltk \n",
    "import numpy as np \n",
    "\n",
    "documents = [\n",
    "    \"The quick brown fox jumped over the lazy dog.\",\n",
    "    \"The dog barked at the moon.\",\n",
    "    \"The fox and the dog are good friends.\"\n",
    "]\n",
    "\n",
    "# Create a CountVectorizer instance\n",
    "#The CountVectorizer from scikit-learn is used to convert a collection of text documents into a matrix of token counts\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# Fit the vectorizer to the documents and transform them into a bag-of-words representation\n",
    "#The fit_transform method fits the model to the documents and transforms the documents into a bag-of-words matrix.\n",
    "bow_matrix = vectorizer.fit_transform(documents)\n",
    "\n",
    "\n",
    "# Get the feature names (words) in the bag-of-words representation\n",
    "#The get_feature_names_out method returns the feature names (words) in the bag-of-words representation.\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "\n",
    "# Print the bag-of-words matrix and feature names\n",
    "print(\"Bag-of-Words Matrix:\")\n",
    "print(bow_matrix.toarray())\n",
    "print(\"\\nFeature Names:\")\n",
    "print(feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenization: in Natural Language Processing (NLP) and machine learning, refers to the process of converting a sequence of text into smaller parts, known as tokens. \n",
    "\n",
    "These tokens can be as small as characters or as long as words. The primary reason this process matters is that it helps machines understand human language by breaking it down into bite-sized pieces, which are easier to analyze."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download NLTK resources (you only need to do this once)\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Tokenization', 'is', 'important', 'for', 'NLP', '.', 'It', 'helps', 'machines', 'understand', 'documents', 'and', 'text', 'by', 'breaking', 'it', 'down']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Input text\n",
    "text = \"Tokenization is important for NLP. It helps machines understand documents and text by breaking it down\"\n",
    "\n",
    "# Tokenize the text into words\n",
    "tokens = word_tokenize(text)\n",
    "\n",
    "# Print the tokens\n",
    "print(tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatization   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explayin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tfidf   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "words please"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Named entity recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "feed words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Sentiment analysis\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
