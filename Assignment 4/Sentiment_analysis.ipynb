{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import nltk\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Topic detection\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RegExps "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RegExp: Stands for Regular Expression, it is way to analyze a sequence of characters and find patterns. \n",
    "RegEx can be used to check if a string contains the specified search pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "YES! We have a match!\n",
      "['ai', 'ai', 'ai', 'ai']\n"
     ]
    }
   ],
   "source": [
    "# import the necessary library\n",
    "import re\n",
    "\n",
    "#create a text where we will analyse and find patterns\n",
    "text = \"Its raining here in Spain. The air feels cold. hope we can sail today\"\n",
    "\n",
    "#Search in the text if \"ai\" can be found\n",
    "x = re.findall(\"ai\", text)\n",
    "\n",
    "\n",
    "#Search the string to see if it starts with \"Its\" and ends with \"today\":\n",
    "z = re.search(\"^Its.*today$\", text) \n",
    "\n",
    "# In case we have a match print\n",
    "if z:\n",
    "  print(\"YES! We have a match!\")\n",
    "else:\n",
    "  print(\"No match\")\n",
    "\n",
    "#print the list where the information was stored\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bow  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bag of Words is a method to extract features from text documents.This model is used to preprocess the text by converting it into a bag of words, which keeps a count of the total occurrences of most frequently used words.\n",
    "\n",
    "These features can be used for training machine learning algorithms. It creates a vocabulary of all the unique words occurring in all the documents in the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bag-of-Words Matrix:\n",
      "[[0 0 0 0 1 1 1 0 0 1 1 0 1 1 2]\n",
      " [0 0 1 1 0 1 0 0 0 0 0 1 0 0 2]\n",
      " [1 1 0 0 0 1 1 1 1 0 0 0 0 0 2]]\n",
      "\n",
      "Feature Names:\n",
      "['and' 'are' 'at' 'barked' 'brown' 'dog' 'fox' 'friends' 'good' 'jumped'\n",
      " 'lazy' 'moon' 'over' 'quick' 'the']\n"
     ]
    }
   ],
   "source": [
    "# Import the necessary library\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import nltk \n",
    "import numpy as np \n",
    "\n",
    "documents = [\n",
    "    \"The quick brown fox jumped over the lazy dog.\",\n",
    "    \"The dog barked at the moon.\",\n",
    "    \"The fox and the dog are good friends.\"\n",
    "]\n",
    "\n",
    "# Create a CountVectorizer instance\n",
    "#The CountVectorizer from scikit-learn is used to convert a collection of text documents into a matrix of token counts\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# Fit the vectorizer to the documents and transform them into a bag-of-words representation\n",
    "#The fit_transform method fits the model to the documents and transforms the documents into a bag-of-words matrix.\n",
    "bow_matrix = vectorizer.fit_transform(documents)\n",
    "\n",
    "\n",
    "# Get the feature names (words) in the bag-of-words representation\n",
    "#The get_feature_names_out method returns the feature names (words) in the bag-of-words representation.\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "\n",
    "# Print the bag-of-words matrix and feature names\n",
    "print(\"Bag-of-Words Matrix:\")\n",
    "print(bow_matrix.toarray())\n",
    "print(\"\\nFeature Names:\")\n",
    "print(feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenization: in Natural Language Processing (NLP) and machine learning, refers to the process of converting a sequence of text into smaller parts, known as tokens. \n",
    "\n",
    "These tokens can be as small as characters or as long as words. The primary reason this process matters is that it helps machines understand human language by breaking it down into bite-sized pieces, which are easier to analyze."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download NLTK resources (you only need to do this once)\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Tokenization', 'is', 'important', 'for', 'NLP', '.', 'It', 'helps', 'machines', 'understand', 'documents', 'and', 'text', 'by', 'breaking', 'it', 'down']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Input text\n",
    "text = \"Tokenization is important for NLP. It helps machines understand documents and text by breaking it down\"\n",
    "\n",
    "# Tokenize the text into words\n",
    "tokens = word_tokenize(text)\n",
    "\n",
    "# Print the tokens\n",
    "print(tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatization   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lemmatization: It is the process of grouping together the different inflected forms of a word so they can be analyzed as a single item.\n",
    "It works by linking words with similar meanings to one word.  reducing words to their base or root form, known as the lemma. \n",
    "\n",
    "For example, the lemma of the words \"running,\" \"ran,\" and \"runs\" is \"run.\" Lemmatization is often used in natural language processing (NLP) to standardize words and reduce them to their base form for better analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download NLTK resources (you only need to do this once)\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rocks : rock\n",
      "corpora : corpus\n",
      "better : good\n",
      "running : run\n",
      "largest : large\n",
      "trying : try\n"
     ]
    }
   ],
   "source": [
    "# Import necessary library\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "#Here, an instance of the WordNetLemmatizer is created. This object (lemmatizer) will be used to perform lemmatization on words.\n",
    "lemmatizer = WordNetLemmatizer()\n",
    " \n",
    "#This line prints the result of lemmatizing the word \"rocks\" using the lemmatizer. \n",
    "# The output will be \"rock\" because \"rocks\" is lemmatized to its base form.\n",
    "\n",
    "print(\"rocks :\", lemmatizer.lemmatize(\"rocks\"))\n",
    "print(\"corpora :\", lemmatizer.lemmatize(\"corpora\"))\n",
    " \n",
    "#This line prints the result of lemmatizing the word \"better\" with the additional information \n",
    "# that it is an adjective (denoted by the pos=\"a\" argument). \n",
    "# The output will be \"good\" because the lemma of the comparative adjective \"better\" is \"good.\"\n",
    "# a denotes adjective in \"pos\"\n",
    "print(\"better :\", lemmatizer.lemmatize(\"better\", pos =\"a\"))\n",
    "\n",
    "\n",
    "#The output will be \"run\" because \"running\" is the present participle form of the verb, and lemmatization reduces it to the base form\n",
    "print(\"running :\", lemmatizer.lemmatize(\"running\", pos=\"v\"))\n",
    "\n",
    "\n",
    "print(\"largest :\", lemmatizer.lemmatize(\"largest\", pos=\"a\"))\n",
    "\n",
    "print(\"trying :\", lemmatizer.lemmatize(\"trying\", pos=\"v\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF- IDF   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Term Frequency - Inverse Document Frequency (TF-IDF)** is a widely used statistical method in natural language processing and information retrieval. It measures how important a term is within a document relative to a collection of documents (i.e., relative to a corpus). Words within a text document are transformed into importance numbers by a text vectorization process. There are many different text vectorization scoring schemes, with TF-IDF being one of the most common."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only need to download once\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>and</th>\n",
       "      <th>document</th>\n",
       "      <th>first</th>\n",
       "      <th>is</th>\n",
       "      <th>one</th>\n",
       "      <th>second</th>\n",
       "      <th>the</th>\n",
       "      <th>third</th>\n",
       "      <th>this</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.469791</td>\n",
       "      <td>0.580286</td>\n",
       "      <td>0.384085</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.384085</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.384085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.687624</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.281089</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.538648</td>\n",
       "      <td>0.281089</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.281089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.511849</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.267104</td>\n",
       "      <td>0.511849</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.267104</td>\n",
       "      <td>0.511849</td>\n",
       "      <td>0.267104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.469791</td>\n",
       "      <td>0.580286</td>\n",
       "      <td>0.384085</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.384085</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.384085</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        and  document     first        is       one    second       the  \\\n",
       "0  0.000000  0.469791  0.580286  0.384085  0.000000  0.000000  0.384085   \n",
       "1  0.000000  0.687624  0.000000  0.281089  0.000000  0.538648  0.281089   \n",
       "2  0.511849  0.000000  0.000000  0.267104  0.511849  0.000000  0.267104   \n",
       "3  0.000000  0.469791  0.580286  0.384085  0.000000  0.000000  0.384085   \n",
       "\n",
       "      third      this  \n",
       "0  0.000000  0.384085  \n",
       "1  0.000000  0.281089  \n",
       "2  0.511849  0.267104  \n",
       "3  0.000000  0.384085  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "\n",
    "# Sample documents\n",
    "documents = [\n",
    "    \"This is the first document.\",\n",
    "    \"This document is the second document.\",\n",
    "    \"And this is the third one.\",\n",
    "    \"Is this the first document?\",\n",
    "]\n",
    "\n",
    "# Instructing the vectorizer to ignore common English words that typically don't contain much information.\n",
    "vectorizer = TfidfVectorizer(stop_words=stopwords.words('english'))\n",
    "\n",
    "# Create a TfidfVectorizer instance\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Fit and transform the documents -- This is a sparse matrix representation of the documents after TF-IDF transformation.\n",
    "tfidf_matrix = vectorizer.fit_transform(documents)\n",
    "\n",
    "\n",
    "# Get feature names (words) and TF-IDF values\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "tfidf_values = tfidf_matrix.toarray()\n",
    "\n",
    "#Convert the matrix to a dense format and put it in a DataFrame\n",
    "import pandas as pd\n",
    "tfidf_df = pd.DataFrame(tfidf_matrix.todense(), columns=feature_names)\n",
    "\n",
    "tfidf_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Named entity recognition (NER)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NER identifies, categorizes and extracts the most important pieces of information from unstructured text without requiring time-consuming human analysis. It's particularly useful for quickly extracting key information from large amounts of data because it automates the extraction process.\n",
    "\n",
    "\n",
    "Its used in many fields in **artificial intelligence (AI)**, including *machine learning (ML)*, *deep learning* and *neural networks*. NER is a key component of NLP systems, such as chatbots, sentiment analysis tools and search engines. It's used in healthcare, finance, human resources (HR), customer support, higher education and social media analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple ORG\n",
      "Cupertino GPE\n",
      "California GPE\n",
      "Five CARDINAL\n",
      "U.S. GPE\n",
      "Amazon ORG\n",
      "Google ORG\n",
      "Microsoft ORG\n",
      "Meta ORG\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# Load the spaCy model for English NER\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Define the text to analyze\n",
    "text = \"Apple is a multinational technology company headquartered in Cupertino, California, that designs, develops, and sells consumer electronics, computer software, and online services. It is considered one of Big Five technology companies in the U.S. information technology industry, along with Amazon, Google, Microsoft, and Meta.\"\n",
    "\n",
    "# Analyze the text using spaCy's NER\n",
    "doc = nlp(text)\n",
    "\n",
    "# Extract named entities and their labels\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Sentiment analysis\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **N-grams**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **What are n-grams?**\n",
    "\n",
    "N-grams are contiguous sequences of n items from a given sequence of text or speech. The value of n determines the length of the n-gram. For instance, bigrams are sequences of two adjacent items, while trigrams are sequences of three adjacent items. N-grams are widely used in NLP tasks such as language modeling, machine translation, and speech recognition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('The', 'quick')\n",
      "('quick', 'brown')\n",
      "('brown', 'fox')\n",
      "('fox', 'jumps')\n",
      "('jumps', 'over')\n",
      "('over', 'the')\n",
      "('the', 'lazy')\n",
      "('lazy', 'dog.')\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.util import ngrams\n",
    "\n",
    "# Sample text corpus\n",
    "corpus = \"The quick brown fox jumps over the lazy dog.\"\n",
    "\n",
    "# Generate bigrams from the corpus\n",
    "bigrams = ngrams(corpus.split(), 2)\n",
    "\n",
    "# Print the generated bigrams\n",
    "for bigram in bigrams:\n",
    "    print(bigram)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **What might you use n-grams for?**\n",
    "\n",
    "N-grams are a powerful tool for analyzing and understanding the structure of language. They can be used to identify patterns in word usage, predict next words in a sequence, and develop language models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('The', 'quick')\n",
      "('quick', 'brown')\n",
      "('brown', 'fox')\n",
      "('fox', 'jumps')\n",
      "('jumps', 'over')\n",
      "('over', 'the')\n",
      "('the', 'lazy')\n",
      "('lazy', 'dog.')\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.util import ngrams\n",
    "\n",
    "# Sample text corpus\n",
    "corpus = \"The quick brown fox jumps over the lazy dog.\"\n",
    "\n",
    "# Define the n-gram order\n",
    "n = 2\n",
    "\n",
    "# Generate n-grams from the corpus\n",
    "n_grams = ngrams(corpus.split(), n)\n",
    "\n",
    "# Print the n-grams\n",
    "for gram in n_grams:\n",
    "    print(gram)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Why would there be a risk for overfitting the data with n-grams**\n",
    "\n",
    "Here are some reasons why with n-grams it can be a risk to overfitting ->\n",
    "\n",
    "1. **High dimensionality**: N-grams, especially higher-order n-grams, can result in a large number of features, making the model more complex and prone to overfitting.\n",
    "\n",
    "2. **Data sparsity**: As the order of n-grams increases, the frequency of each n-gram decreases, leading to sparse data. This sparsity can make it difficult for the model to generalize well to unseen data.\n",
    "\n",
    "3. **Noise sensitivity**: N-grams can capture noise and idiosyncrasies in the training data, leading to overfitting. This is because n-grams treat all sequences of words equally, regardless of their meaning or relevance.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Here is an example of how you can prevent overfitting while using n-grams**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.0\n",
      "Test Tokens: ['quick', 'jumps']\n",
      "Predicted Tokens: ['brown' 'dog']\n",
      "Intersection: set()\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Tokenize the corpus\n",
    "corpus = \"The quick brown fox jumps over the lazy dog.\"\n",
    "tokens = word_tokenize(corpus)\n",
    "\n",
    "# Generate bigrams from the tokens\n",
    "bigrams = list(nltk.ngrams(tokens, 2))\n",
    "\n",
    "# Split the data into train and test sets for tokens and bigrams\n",
    "X_train_tokens, X_test_tokens, y_train_tokens, y_test_tokens = train_test_split(tokens[:-1], tokens[1:], test_size=0.2)\n",
    "X_train_bigrams, X_test_bigrams, _, _ = train_test_split(bigrams, tokens[1:], test_size=0.2)\n",
    "\n",
    "# Convert bigrams to strings\n",
    "X_train_strings = [\" \".join(bigram) for bigram in X_train_bigrams]\n",
    "X_test_strings = [\" \".join(bigram) for bigram in X_test_bigrams]\n",
    "\n",
    "# Vectorize the data\n",
    "vectorizer = CountVectorizer()\n",
    "X_train_vec = vectorizer.fit_transform(X_train_strings)\n",
    "X_test_vec = vectorizer.transform(X_test_strings)\n",
    "\n",
    "# Train the Multinomial Naive Bayes classifier\n",
    "classifier = MultinomialNB()\n",
    "classifier.fit(X_train_vec, y_train_tokens)\n",
    "\n",
    "# Evaluate the classifier\n",
    "y_pred = classifier.predict(X_test_vec)\n",
    "accuracy = accuracy_score(y_test_tokens, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Test Tokens:\", y_test_tokens)\n",
    "print(\"Predicted Tokens:\", y_pred)\n",
    "intersection = set(y_test_tokens) & set(y_pred)\n",
    "print(\"Intersection:\", intersection)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above code uses *data splitting, L2 regularization, feature selection and early stopping* to be able to use n-grams without it overfitting.\n",
    "\n",
    "Other tecniques you can use are:\n",
    "\n",
    "- Data augmentation\n",
    "\n",
    "- Feature hashing\n",
    "\n",
    "- Model complexity control"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Earlier, we basically wanted to get rid off punctuations, but here, why we might want to use them here?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Punctuation marks can serve valuable purposes in n-gram analysis, despite the initial inclination to remove them. While removing punctuation can streamline the process and reduce the number of n-grams, **it can also overlook important contextual information.** Punctuation marks play a significant role in human language, and their consideration can enhance the effectiveness of n-gram-based applications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some examples:\n",
    "\n",
    "1. **Sentence Structure and Grammar**: Punctuation marks provide cues about sentence structure and grammar, allowing for more accurate identification of meaningful n-grams. For instance, a full stop indicates the end of a sentence, enabling the recognition of n-grams that span entire sentences.\n",
    "\n",
    "2. **Discourse Coherence**: Punctuation marks contribute to discourse coherence by signaling relationships between clauses and phrases. This information is crucial for understanding the context and meaning of n-grams that extend across multiple clauses.\n",
    "\n",
    "3. **Sentiment Analysis**: Punctuation marks can convey emotional tones and sentiments, which can be particularly useful in sentiment analysis tasks. For example, exclamation points and question marks often indicate surprise or inquiry, while ellipses suggest pauses or unspoken thoughts.\n",
    "\n",
    "4. **Preserving Author's Intent**: Punctuation marks can help preserve the author's intended meaning and tone, especially in creative writing or informal communication. Removing punctuation can alter the interpretation and impact of the text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Thank you!\n",
    "\n",
    "Group 2:\n",
    "\n",
    "Edem Quashigah\n",
    "\n",
    "Catarina Kaucher\n",
    "\n",
    "Sara-Sofia Paananen"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
